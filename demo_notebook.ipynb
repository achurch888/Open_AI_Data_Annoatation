{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e0e8216",
   "metadata": {},
   "source": [
    "# Demo Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e9fc27",
   "metadata": {},
   "source": [
    "This notebook is a demo of the methodology used to retrieve labels for qualitative data using OpenAI's python API. This notebook uses data from the associated paper \"Using GPT for Supervised Content Coding: An Application in Corresponding Experiments\", but the method described is applicable to many use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1344002",
   "metadata": {},
   "outputs": [],
   "source": [
    "#install and update openai api and tokenizer\n",
    "#!pip install openai\n",
    "#!pip install --upgrade openai\n",
    "#!pip install tiktoken\n",
    "#!pip install --upgrade tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f2d6245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import tiktoken as tk #tokenizer\n",
    "\n",
    "#import exponential backoff\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    ")\n",
    "\n",
    "client = OpenAI(\n",
    "  api_key='your_api_key',  # this is also the default, it can be omitted\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ca64cc",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9b1230",
   "metadata": {},
   "source": [
    "The below function is used to pass prompts to gpt. We use chat completions to run newer models such as gpt-3.5-turbo and gpt-4. Completions are also available for older models. This function also uses exponential backoff. Large jobs will often fail due to one request being denied by OpenAI. Exponential backoff runs the failed request again after a specified wait period, ensuring that a complete job will get through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1842f104",
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6)) #exponential backoff\n",
    "def run_chat_complete_job(prompt, model):\n",
    "    \"\"\"\n",
    "    Pass a prompt to OpenAI using a chat completion. Works with gpt-3.5-turbo and gpt-4.\n",
    "    \n",
    "    Parameters:\n",
    "    - prompt (str): user prompt\n",
    "    - model (str): gpt model to be used\n",
    "    \n",
    "    Returns:\n",
    "    - dict: completion from OpenAI\n",
    "    \"\"\"\n",
    "    \n",
    "    #send chat completion job\n",
    "    completion = client.chat.completions.create(\n",
    "    model = model, #specify the model\n",
    "    messages=[\n",
    "        {'role':'system', 'content': 'You help interpret emails.',\n",
    "        'role': 'user', 'content': prompt}\n",
    "    ],\n",
    "    temperature = 0, #randomness of completion, 0 returns the same completion every time\n",
    "    max_tokens = 1, #number of tokens to return in the completion\n",
    "    )\n",
    "    \n",
    "    return completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4c0569",
   "metadata": {},
   "source": [
    "#### Bootstrapping Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb88de87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_accuracy(df, gpt_response, human_response, n):\n",
    "    \"\"\"\n",
    "    Generate a bootstrap distribution of accuracies.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (dataframe): dataframe of labeled examples\n",
    "    - gpt_response (str): gpt response column\n",
    "    - human_label (str): human label column\n",
    "    - n (int): number of bootstrap samples\n",
    "    \n",
    "    Returns:\n",
    "    - numpy array: array of accuracies from each bootstrapped sample\n",
    "    \"\"\"\n",
    "    \n",
    "    sample_accuracy = np.empty(n) #array to store bootstrapped accuracies\n",
    "    \n",
    "    #loop throough n, generate sample, record accuracy\n",
    "    for i in range(n):\n",
    "        boot_sample = df.sample(n=len(df), replace=True)\n",
    "        accuracy = round((np.sum(np.where(boot_sample[gpt_response] == boot_sample[human_response], 1, 0)))/len(boot_sample), 2)\n",
    "        sample_accuracy[i] = accuracy\n",
    "        \n",
    "    return sample_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b7c33e",
   "metadata": {},
   "source": [
    "The below function estimates the price of running a job through OpenAI using gpt-3.5-turbo or gpt-4. The price is calculated by multiplying the total number of prompt tokens (in our case, the base prompt plus the emails to be attached to each prompt) by the input token price, plus the output token price times the number of tokens in the output. Prices from OpenAI are quoted in price per thousand tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c01a2746",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gpt prices per thousand tokens\n",
    "gpt_input_price = {'gpt-3.5-turbo': 0.0015, 'gpt-4': 0.03}\n",
    "gpt_output_price = {'gpt-3.5-turbo': 0.002, 'gpt-4': 0.06}\n",
    "\n",
    "def calculate_job_price(prompt, text, model, output_size):\n",
    "    \"\"\"\n",
    "    Takes in a prompt, a dataframe with text to be labeled, the model to be used, and the size of the gpt response\n",
    "    and returns the cost of running the job in dollars.\n",
    "    \n",
    "    Parameters:\n",
    "    - prompt (str): prompt to be passed to gpt\n",
    "    - text (series, list): a series or list containing text to be labeled.\n",
    "    - model (str): gpt model to be used\n",
    "    - output_size (int): the number of tokens gpt will use in completions\n",
    "    \n",
    "    Returns:\n",
    "    - a dictionary containing the price, number of tokens in the prompt, number of tokens in the text, the total\n",
    "      input tokens, and the total output tokens.\n",
    "    \"\"\"\n",
    "    \n",
    "    encoding = tk.encoding_for_model(model) #initialize an encoding object\n",
    "    \n",
    "    total_labels = len(text) #total number of labels needs\n",
    "    prompt_tokens = len(encoding.encode(prompt)) #number of tokens for the prompt and the text\n",
    "    text_tokens = len(encoding.encode(' '.join(text))) #number of tokens contributed by text\n",
    "    total_input_tokens = text_tokens + (prompt_tokens * total_labels)\n",
    "    total_output_tokens = total_labels * output_size\n",
    "    \n",
    "    #calculate the total price\n",
    "    price = round((total_input_tokens/1000) * gpt_input_price[model] \n",
    "                  + (total_output_tokens/1000) * gpt_output_price[model], 2)\n",
    "    \n",
    "    return {'price':price, 'prompt_tokens':prompt_tokens, \n",
    "            'text_tokens':text_tokens, 'total_input_tokens':total_input_tokens, \n",
    "            'total_output_tokens':total_output_tokens}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c863ed5",
   "metadata": {},
   "source": [
    "The below function prepares a list of shots to be passed to gpt along with the prompt. Shots are pre-labeled examples passed as part of a prompt used to provide guidance to gpt models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa9184d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_shots(df, text_column, human_label_column, shots):\n",
    "    \"\"\"\n",
    "    Prepares shots to be used in a prompt when shots are contained within the test dataframe.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (dataframe): dataframe containing text to be labeled\n",
    "    - text_column (str): the column of the dataframe containing the text to be labeled\n",
    "    - human_label_column (str): column containing the human label\n",
    "    - shots (list of integers): list of indices to be used as shots\n",
    "    \n",
    "    Returns:\n",
    "    - tuple containing a list of shots, and a copy of df with the examples removed. \n",
    "    \"\"\"\n",
    "    \n",
    "    #grab the shots from the original dataframe\n",
    "    df_shots = df.loc[shots, [text_column, human_label_column]]\n",
    "    \n",
    "    #store the shots in a list, staggering the email and the label\n",
    "    message = list(df_shots[text_column])\n",
    "    response = list(df_shots[human_label_column])\n",
    "    intermediate = list(map(list, zip(message, response)))\n",
    "    shots_list = [a for sublist in intermediate for a in sublist]\n",
    "    \n",
    "    #remove the shots from the original dataframe\n",
    "    df_shotless = df[~df.index.isin(shots)].reset_index(drop=True)\n",
    "    \n",
    "    return shots_list, df_shotless"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e49bdc6",
   "metadata": {},
   "source": [
    "# Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff50f30",
   "metadata": {},
   "source": [
    "Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199838ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"I am providing emails from a nursing home in response to a potential resident.\n",
    "            If the email indicates that the nursing home has availability, respond with yes.\n",
    "            If the email indicates that the nursing home does not have availability,\n",
    "            respond with no. If the email indicates that the nursing home cannot accept\n",
    "            the client, respond with no. If the email mentions the word waitlist,\n",
    "            respond with waitlist. If the email does not indicate any of the above,\n",
    "            respond with nan.\n",
    "\n",
    "            email:{}\n",
    "            response:\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb41b11b",
   "metadata": {},
   "source": [
    "Payable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a1afe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"I am providing an email from a nursing home in response to a potential client.\n",
    "            If the email mentions payment or insurance, respond with yes.\n",
    "            Otherwise, respond with no.\n",
    "\n",
    "            email:{}\n",
    "            response:\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee790aa",
   "metadata": {},
   "source": [
    "Citizen Flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044375f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"I am providing an email from a nursing home in response to a potential client.\n",
    "          If the email mentions citizenship or citizenship restrictions, respond with yes.\n",
    "          Otherwise, respond with no.\n",
    "            \n",
    "          email:{}\n",
    "          response:\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8193fe",
   "metadata": {},
   "source": [
    "More Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0db47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"I am providing an email from a nursing home in response to a potential client.\n",
    "            If the email requests additional information, documentation, or paperwork from the client,\n",
    "            respond with yes. If the email asks the client a question, respond with yes. Otherwise,\n",
    "            respond with no.\n",
    "                \n",
    "            email:{}\n",
    "            response:\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd80df2",
   "metadata": {},
   "source": [
    "## Test Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e20952",
   "metadata": {},
   "source": [
    "This section shows how prompts can be tested on a subset of the data before being used on the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ba0f76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/achurch/Desktop/RA_Job/actual_final_stuff/experiment_data/final_clean_results.csv'\n",
    "\n",
    "df = pd.read_csv(path, index_col = 0, dtype=str)\n",
    "\n",
    "#replace binary responses with yes and no\n",
    "df.replace({'1':'yes', '0':'no'}, inplace=True)\n",
    "df['availableGPT'] = df['availableGPT'].astype(str)\n",
    "df['available'] = df['available'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6cbaac",
   "metadata": {},
   "source": [
    "#### Available, No Shots, gpt-3.5-turbo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccd7d1d",
   "metadata": {},
   "source": [
    "Calculate price and define variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74fa94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a random sample from the dataframe\n",
    "df_test = df.sample(100)\n",
    "\n",
    "#define the input variables\n",
    "prompt = \"\"\"I am providing emails from a nursing home in response to a potential resident.\n",
    "            If the email indicates that the nursing home has availability, respond with yes.\n",
    "            If the email indicates that the nursing home does not have availability,\n",
    "            respond with no. If the email indicates that the nursing home cannot accept\n",
    "            the client, respond with no. If the email mentions the word waitlist,\n",
    "            respond with waitlist. If the email does not indicate any of the above,\n",
    "            respond with nan.\n",
    "\n",
    "            email:{}\n",
    "            response:\"\"\"\n",
    "            \n",
    "model = 'gpt-3.5-turbo'\n",
    "\n",
    "#calculate price before running\n",
    "calculate_job_price(prompt, df_test['email_message'], model, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfa4c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment to run a job\n",
    "#df_test['GPT'] = df_test['email_message'].apply(lambda x: run_chat_complete_job(prompt.format(x), model))\n",
    "\n",
    "#retrieve completions from the gpt response and clean it\n",
    "df_test['response'] = df_test['GPT'].apply(lambda x: x.choices[0].message.content)\n",
    "df_test['response'] = df_test['response'].apply(lambda x: x.strip().lower())\n",
    "\n",
    "#for availability, if model responds with wait, change to waitlist, result of max_tokens = 1\n",
    "df_test['response'] = df_test['response'].apply(lambda x: 'waitlist' if x == 'wait' else x)\n",
    "\n",
    "#calculate the accuracy of compared to the human responses\n",
    "accuracy = np.sum(df_test['response'] == df_test['available']) / len(df_test)\n",
    "print(round(accuracy, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d944a4",
   "metadata": {},
   "source": [
    "Now bootstrap the test, sampling with replacement and calculating the accuracy as many times as is comuptationally feasible. In this case we calculate accuracy 1000 times. Then calculate the confidence interval. The confidence interval provides a range of values that we can be 95% sure contains the accuracy that will be achieved on the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d052c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bootstrap accuracy, proportion of review label\n",
    "bootstrap_dist = bootstrap_accuracy(df_test, 'response', 'available', 1000)\n",
    "\n",
    "#confidence interval using the percentile method\n",
    "ci_info = (np.percentile(bootstrap_dist, 2.5), np.percentile(bootstrap_dist, 97.5))\n",
    "\n",
    "print('sample_accuracy: ', round(bootstrap_dist.mean(), 2))\n",
    "print('confidence_interval: ', ci_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc69e7b1",
   "metadata": {},
   "source": [
    "#### Payable with Shots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422581d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the input variables\n",
    "prompt = \"\"\"I am providing an email from a nursing home in response to a potential client.\n",
    "            If the email mentions payment or insurance, respond with yes.\n",
    "            Otherwise, respond with no.\n",
    "\n",
    "            email:{}\n",
    "            response:{}\n",
    "            email:{}\n",
    "            response:{}\n",
    "            email:\n",
    "            response:\"\"\"\n",
    "\n",
    "model = 'gpt-3.5-turbo'\n",
    "\n",
    "#choose shots and remove from orginal dataframe\n",
    "shots = [25, 46]\n",
    "shots_list, df_shotless = prepare_shots(df, 'email_message', 'payable', shots)\n",
    "\n",
    "#create a sample from the dataframe without shots\n",
    "df_test = df_shotless.sample(100)\n",
    "\n",
    "#calculate price before running\n",
    "calculate_job_price(prompt.format(*shots_list), df_test['email_message'], model, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee3c046",
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment to run a job\n",
    "#df_test['GPT'] = df_test['email_message'].apply(lambda x: run_chat_complete_job(prompt.format(*shots_list, x), model))\n",
    "\n",
    "#retrieve completions from the gpt response and clean it\n",
    "df_test['response'] = df_test['GPT'].apply(lambda x: x.choices[0].message.content)\n",
    "df_test['response'] = df_test['response'].apply(lambda x: x.strip().lower())\n",
    "\n",
    "#calculate the accuracy of compared to the human responses\n",
    "accuracy = np.sum(df_test['response'] == df_test['payable']) / len(df_test)\n",
    "print(round(accuracy, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20b4132",
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrap_dist = bootstrap_accuracy(df_test, 'response', 'payable', 1000)\n",
    "\n",
    "#confidence interval using the percentile method\n",
    "ci_info = (np.percentile(bootstrap_dist, 2.5), np.percentile(bootstrap_dist, 97.5))\n",
    "\n",
    "print('sample_accuracy: ', round(bootstrap_dist.mean(), 2))\n",
    "print('confidence_interval: ', ci_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9865c120",
   "metadata": {},
   "source": [
    "Once a satisfactory confidence interval is achieved, the job can be run on the entire dataset using the same method as above, passing in the entire dataset in place of the sample."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
